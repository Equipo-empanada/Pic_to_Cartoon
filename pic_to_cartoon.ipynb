{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import itertools\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) #Rtx 2070 super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial convolution block\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "        \n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        \n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.Conv2d(64, output_nc, kernel_size=7, stride=1, padding=3),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files = sorted(os.listdir(root))\n",
    "        self.root = root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root, self.files[index])\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(256 * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Crear los datasets y dataloaders\n",
    "batch_size = 1\n",
    "\n",
    "celeba_dataset = ImageDataset(root='./datasets/img_align_celeba/img_align_celeba/', transform=transform)\n",
    "caricature_dataset = ImageDataset(root='./datasets/cartoonset10k/', transform=transform)\n",
    "\n",
    "celeba_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "caricature_loader = DataLoader(caricature_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar los generadores y discriminadores\n",
    "G_A2B = Generator(input_nc=3, output_nc=3).to(device)\n",
    "G_B2A = Generator(input_nc=3, output_nc=3).to(device)\n",
    "D_A = Discriminator(input_nc=3).to(device)\n",
    "D_B = Discriminator(input_nc=3).to(device)\n",
    "\n",
    "# Inicializar los pesos\n",
    "G_A2B.apply(weights_init_normal)\n",
    "G_B2A.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)\n",
    "\n",
    "# Definir las pérdidas y optimizadores\n",
    "criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_cycle = torch.nn.L1Loss().to(device)\n",
    "criterion_identity = torch.nn.L1Loss().to(device)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_A, real_B) in enumerate(zip(celeba_loader, caricature_loader)):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # Generadores A2B y B2A\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identidad\n",
    "        loss_id_A = criterion_identity(G_B2A(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_A2B(real_B), real_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = G_A2B(real_A)\n",
    "        pred_fake = D_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, torch.ones_like(pred_fake, device=device))\n",
    "\n",
    "        fake_A = G_B2A(real_B)\n",
    "        pred_fake = D_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, torch.ones_like(pred_fake, device=device))\n",
    "\n",
    "        loss_GAN = (loss_GAN_A2B + loss_GAN_B2A) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recovered_A = G_B2A(fake_B)\n",
    "        loss_cycle_A = criterion_cycle(recovered_A, real_A)\n",
    "\n",
    "        recovered_B = G_A2B(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recovered_B, real_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + 10.0 * loss_cycle + 5.0 * loss_identity\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Discriminadores A y B\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        pred_real = D_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real, device=device))\n",
    "\n",
    "        pred_fake = D_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake, device=device))\n",
    "\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) / 2\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        pred_real = D_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real, device=device))\n",
    "\n",
    "        pred_fake = D_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake, device=device))\n",
    "\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) / 2\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        # Imprimir las pérdidas\n",
    "        print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(celeba_loader)}] \"\n",
    "              f\"[D loss: {loss_D_A.item() + loss_D_B.item()}] \"\n",
    "              f\"[G loss: {loss_G.item()}]\")\n",
    "        \n",
    "        # Guardar imágenes generadas y modelos\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_B = G_A2B(real_A)\n",
    "            fake_A = G_B2A(real_B)\n",
    "            save_image(fake_B, f\"output/fake_B_{epoch}.png\", normalize=True)\n",
    "            save_image(fake_A, f\"output/fake_A_{epoch}.png\", normalize=True)\n",
    "\n",
    "        torch.save(G_A2B.state_dict(), f\"models/G_A2B_{epoch}.pth\")\n",
    "        torch.save(G_B2A.state_dict(), f\"models/G_B2A_{epoch}.pth\")\n",
    "        torch.save(D_A.state_dict(), f\"models/D_A_{epoch}.pth\")\n",
    "        torch.save(D_B.state_dict(), f\"models/D_B_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo\n",
    "def evaluate_model(generator_A2B, generator_B2A, test_loader_A, test_loader_B, device):\n",
    "    generator_A2B.eval()\n",
    "    generator_B2A.eval()\n",
    "\n",
    "    for i, (real_A, real_B) in enumerate(zip(test_loader_A, test_loader_B)):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fake_B = generator_A2B(real_A)\n",
    "            fake_A = generator_B2A(real_B)\n",
    "\n",
    "        save_image(fake_B, f\"evaluation/fake_B_{i}.png\", normalize=True)\n",
    "        save_image(fake_A, f\"evaluation/fake_A_{i}.png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(G_A2B, G_B2A, celeba_loader, caricature_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
